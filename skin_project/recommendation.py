from fastapi import FastAPI
from pydantic import BaseModel
from dotenv import load_dotenv
from sentence_transformers import SentenceTransformer
from pinecone import Pinecone
from openai import OpenAI
import os
from uuid import uuid4

# âœ… í™˜ê²½ ë³€ìˆ˜ ë¡œë”©
env_path = os.path.join(os.path.dirname(__file__), ".env")
load_dotenv(dotenv_path=env_path)

# âœ… í™˜ê²½ ë³€ìˆ˜ ê°€ì ¸ì˜¤ê¸°
openai_api_key = os.getenv("OPENAI_API_KEY")
pinecone_api_key = os.getenv("PINECONE_API_KEY")
pinecone_env = os.getenv("PINECONE_ENV")
pinecone_index_name = os.getenv("PINECONE_INDEX", "toner")

# âœ… ë””ë²„ê¹… ì¶œë ¥
print("âœ… OPENAI_API_KEY =", openai_api_key[:10] + "..." if openai_api_key else None)
print("ğŸ” PINECONE_API_KEY =", pinecone_api_key[:10] + "..." if pinecone_api_key else None)
print("ğŸ” PINECONE_ENV =", pinecone_env)
print("ğŸ” PINECONE_INDEX =", pinecone_index_name)

# âœ… í•„ìˆ˜ í‚¤ í™•ì¸
if not openai_api_key:
    raise RuntimeError("âŒ OPENAI_API_KEYê°€ í™˜ê²½ë³€ìˆ˜ì—ì„œ ë¡œë”©ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
if not pinecone_api_key or not pinecone_env:
    raise RuntimeError("âŒ PINECONE_API_KEY ë˜ëŠ” PINECONE_ENVê°€ ëˆ„ë½ë˜ì—ˆìŠµë‹ˆë‹¤.")

# âœ… í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=openai_api_key)
pc = Pinecone(api_key=pinecone_api_key, environment=pinecone_env)

if pinecone_index_name not in pc.list_indexes().names():
    raise RuntimeError(f"'{pinecone_index_name}' ì¸ë±ìŠ¤ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")

index = pc.Index(pinecone_index_name)
model = SentenceTransformer("jhgan/ko-sbert-nli")
app = FastAPI()

# âœ… ìƒ˜í”Œ í† ë„ˆ ë¦¬ë·° ì—…ë¡œë“œ
@app.on_event("startup")
async def upload_sample_data():
    print("ğŸš€ Pineconeì— í…ŒìŠ¤íŠ¸ìš© í† ë„ˆ ë°ì´í„° ì—…ë¡œë“œ ì¤‘...")
    try:
        reviews = [
            ("ê±´ì„±", "ë¼ìš´ë“œë© ìì‘ë‚˜ë¬´ í† ë„ˆ", "ì†ë³´ìŠµì´ ì˜ ë˜ê³  ê°ì§ˆ ë¶€ê°ì´ ëœí•´ìš”."),
            ("ì§€ì„±", "ì´ë‹ˆìŠ¤í”„ë¦¬ ë¸”ë£¨ë² ë¦¬ ë¦¬ë°¸ëŸ°ì‹± í† ë„ˆ", "ëˆì ì„ ì—†ì´ ì‚°ëœ»í•´ì„œ ì—¬ë¦„ì— ì¢‹ì•„ìš”."),
            ("ë¯¼ê°ì„±", "ì•„ë²¤ëŠ í† ë„ˆ", "ìê·¹ ì—†ì´ ì§„ì •ë¼ì„œ í”¼ë¶€ ì§„ì •ìš©ìœ¼ë¡œ ë”±ì´ì—ìš”."),
            ("ë³µí•©ì„±", "í•œìœ¨ ì–´ë¦°ì‘¥ í† ë„ˆ", "ìœ ìˆ˜ë¶„ ë°¸ëŸ°ìŠ¤ê°€ ì˜ ë§ê³  í–¥ë„ ìˆœí•´ìš”."),
            ("ê±´ì„±", "ë‹¥í„°ì§€ ë ˆë“œ ë¸”ë ˆë¯¸ì‰¬ í† ë„ˆ", "ë¶‰ì€ê¸° ì™„í™”ì— íš¨ê³¼ ìˆê³  ë³´ìŠµë„ ê´œì°®ì•„ìš”."),
        ]

        for skin_type, product_name, review in reviews:
            embedding = model.encode(review).tolist()
            index.upsert([{
                "id": str(uuid4()),
                "values": embedding,
                "metadata": {
                    "skin_type": skin_type,
                    "product_name": product_name,
                    "review": review
                }
            }])
        print("âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„° ì—…ë¡œë“œ ì™„ë£Œ")
    except Exception as e:
        print("âŒ ìƒ˜í”Œ ì—…ë¡œë“œ ì‹¤íŒ¨:", e)

# âœ… ìš”ì²­ ìŠ¤í‚¤ë§ˆ
class RecommendQuery(BaseModel):
    skin_type: str
    description: str

# âœ… ì„ë² ë”© ìƒì„±
def extract_embedding(text: str) -> list:
    return model.encode(text).tolist()

# âœ… Pinecone ìœ ì‚¬ ë¦¬ë·° ê²€ìƒ‰
def search_pinecone(embedding: list, skin_type: str = None):
    results = index.query(
        vector=embedding,
        top_k=3,
        include_metadata=True,
        filter={"skin_type": skin_type} if skin_type else {}
    )
    return results

# âœ… GPT ì¶”ì²œ ìƒì„±
def generate_recommendation(query, results):
    matches = results.get("matches", [])
    print("ğŸ“Œ matches =", matches)

    if not matches:
        return "ğŸ˜¥ ì¶”ì²œí•  ì œí’ˆì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."

    items = [x.get("metadata", {}) for x in matches]
    prompt = f"""
ë‹¹ì‹ ì€ ì˜¬ë¦¬ë¸Œì˜ AI ì ì¥ì…ë‹ˆë‹¤.
í”¼ë¶€íƒ€ì…: {query['skin_type']}
ì›í•˜ëŠ” ì œí’ˆ íŠ¹ì§•: {query['description']}
í›„ë³´ ì œí’ˆ:
"""
    for i, item in enumerate(items, 1):
        product_name = item.get("product_name", "ì œí’ˆëª… ì—†ìŒ")
        review = item.get("review", "ë¦¬ë·° ì—†ìŒ")
        prompt += f"\n{i}. {product_name} - {review}"

    prompt += "\n\nìœ„ ì •ë³´ë¥¼ ì°¸ê³ í•˜ì—¬ ê³ ê°ì—ê²Œ ë”°ëœ»í•˜ê³  ì¹œê·¼í•œ ë§íˆ¬ë¡œ ì œí’ˆ ì¶”ì²œ ë©˜íŠ¸ë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”. ì´ëª¨ì§€ë„ í¬í•¨í•´ì£¼ì„¸ìš”."

    try:
        chat = client.chat.completions.create(
            model="gpt-3.5-turbo",  # âœ… ë¹„ìš© íš¨ìœ¨ ëª¨ë¸
            messages=[{"role": "user", "content": prompt}]
        )
        return chat.choices[0].message.content
    except Exception as e:
        print("âŒ GPT í˜¸ì¶œ ì˜¤ë¥˜:", e)
        return f"âŒ ì¶”ì²œ ì‹¤íŒ¨: {str(e)}"

# âœ… API ì—”ë“œí¬ì¸íŠ¸
@app.post("/recommend")
def recommend_endpoint(query: RecommendQuery):
    embedding = extract_embedding(f"{query.skin_type} í”¼ë¶€, {query.description}")
    results = search_pinecone(embedding, skin_type=query.skin_type)
    message = generate_recommendation(query.model_dump(), results)
    return {"ì¶”ì²œ ë©˜íŠ¸": message}
